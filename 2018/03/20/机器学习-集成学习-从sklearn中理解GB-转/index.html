<html>
  <head>
    <title>机器学习-集成学习-从sklearn中理解GB-转 - 十一城</title>
    <link href='/images/fav.png' rel='shortcut icon'>
<link href='/atom.xml' rel='alternate' type='application/rss+xml'>

<link rel="stylesheet" href="/css/style.css">
<link rel="stylesheet" href="/css/highlight.css">
<link rel="stylesheet" href="/css/responsive.css">


<script src="/js/jquery.js"></script>
<script src="/js/basics.js"></script>

<meta content='width=device-width, initial-scale=1.0, user-scalable=no' name='viewport'>
<meta content='text/html; charset=utf-8' http-equiv='content-type'>


  <meta name="generator" content="Hexo 6.3.0"></head>
  <body>
    <header>
  <!--<a id='go-back-home' href='/'><img alt='Home' width='53' height='59'></a>-->
  <br>
  <p>十一城</p>
  <p>跬步千里，小流江海。</p>
</header>

    <div id='container'>
      <div class='block'>
  
    <a class='main' href='/'>Home</a>
  
    <a class='main' href='/categories/linux'>Linux</a>
  
    <a class='main' href='/categories/ml'>ML</a>
  
    <a class='main' href='/categories/python'>Python</a>
  
    <a class='main' href='/categories/java'>Java</a>
  
    <a class='main' href='/thoughts'>Thoughts</a>
  
    <a class='main' href='/kmkg'>KmKg</a>
  
    <a class='main' href='/bookcan'>BookCan</a>
  
    <a class='main' href='/links'>Links</a>
  
    <a class='main' href='/about'>About</a>
  
</div>

      <section class='paging'>
  
    <div class='left'>
      <a href='/2018/03/27/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-numpy%E4%B8%8A%E6%89%8B%E6%95%99%E7%A8%8B/'>
        ‹
      </a>
    </div>
  
  
    <div class='right'>
      <a href='/2018/03/20/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-SVMs-%E6%A0%B8%E6%96%B9%E6%B3%95/'>
        ›
      </a>
    </div>
  
</section>

      <div class='content'>
        <section class='post'>
          <h1>
            <div class='date'>2018-03-20</div>
            机器学习-集成学习-从sklearn中理解GB-转
          </h1>
          
          
            <font class="categories">
            &#8226; 分类:
            <a class="categories-link" href="/categories/ml/">ml</a>
            </font>
          
          
             &#8226; 标签:
            <font class="tags">
              <a class="tags-none-link" href="/tags/gb/" rel="tag">gb</a>
            </font>
          
          <hr>


          <blockquote>
<p>本文转载自<a target="_blank" rel="noopener" href="http://blog.csdn.net/sa14023053/article/details/51817650">scikit-learn : GBR (Gradient boosting regression)</a></p>
</blockquote>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>梯度提升回归（Gradient boosting regression，GBR）是一种从它的错误中进行学习的技术。它本质上就是集思广益，集成一堆较差的学习算法进行学习。有两点需要注意： </p>
<ul>
<li>每个学习算法准备率都不高，但是它们集成起来可以获得很好的准确率。</li>
<li>这些学习算法依次应用，也就是说每个学习算法都是在前一个学习算法的错误中学习</li>
</ul>
<h2 id="准备模拟数据"><a href="#准备模拟数据" class="headerlink" title="准备模拟数据"></a>准备模拟数据</h2><p>我们还是用基本的回归数据来演示GBR：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">from sklearn.datasets import make_regression</span><br><span class="line">X, y = make_regression(1000, 2, noise=10)</span><br></pre></td></tr></table></figure>

<h2 id="GBR原理"><a href="#GBR原理" class="headerlink" title="GBR原理"></a>GBR原理</h2><p>GBR算是一种集成模型因为它是一个集成学习算法。这种称谓的含义是指GBR用许多较差的学习算法组成了一个更强大的学习算法：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.ensemble import GradientBoostingRegressor as GBR</span><br><span class="line">gbr = GBR()</span><br><span class="line">gbr.fit(X, y)</span><br><span class="line">gbr_preds = gbr.predict(X)</span><br></pre></td></tr></table></figure>

<p>很明显，这里应该不止一个模型，但是这种模式现在很简明。现在，让我们用基本回归算法来拟合数据当作参照：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import LinearRegression</span><br><span class="line">lr = LinearRegression()</span><br><span class="line">lr.fit(X, y)</span><br><span class="line">lr_preds = lr.predict(X)</span><br></pre></td></tr></table></figure>

<p>有了参照之后，让我们看看GBR算法与线性回归算法效果的对比情况。图像生成可以参照第一章正态随机过程的相关主题，首先需要下面的计算：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">gbr_residuals = y - gbr_preds</span><br><span class="line">lr_residuals = y - lr_preds</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">from matplotlib import pyplot as plt</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">f, ax = plt.subplots(figsize=(7, 5))</span><br><span class="line">f.tight_layout()</span><br><span class="line">ax.hist(gbr_residuals,bins=20,label=&#x27;GBR Residuals&#x27;, color=&#x27;b&#x27;, alpha=.5);</span><br><span class="line">ax.hist(lr_residuals,bins=20,label=&#x27;LR Residuals&#x27;, color=&#x27;r&#x27;, alpha=.5);</span><br><span class="line">ax.set_title(&quot;GBR Residuals vs LR Residuals&quot;)</span><br><span class="line">ax.legend(loc=&#x27;best&#x27;);</span><br></pre></td></tr></table></figure>

<p><img src="http://img.voidcn.com/vcimg/000/005/140/665_c9b_5cc.jpg" alt="这里写图片描述"></p>
<p>看起来好像GBR拟合的更好，但是并不明显。让我们用95%置信区间（Confidence interval,CI）对比一下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.percentile(gbr_residuals, [2.5, 97.5])</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([-17.14322801,  17.05182403])</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.percentile(lr_residuals, [2.5, 97.5])</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([-19.79519628,  20.09744884])</span><br></pre></td></tr></table></figure>

<p>GBR的置信区间更小，数据更集中，因此其拟合效果更好；我们还可以对GBR算法进行一些调整来改善效果。我用下面的例子演示一下，然后在下一节介绍优化方法：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">n_estimators = np.arange(100, 1100, 350)</span><br><span class="line">gbrs = [GBR(n_estimators=n_estimator) for n_estimator in n_estimators]</span><br><span class="line">residuals = &#123;&#125;</span><br><span class="line">for i, gbr in enumerate(gbrs):</span><br><span class="line">    gbr.fit(X, y)</span><br><span class="line">    residuals[gbr.n_estimators] = y - gbr.predict(X)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">f, ax = plt.subplots(figsize=(7, 5))</span><br><span class="line">f.tight_layout()</span><br><span class="line">colors = &#123;800:&#x27;r&#x27;, 450:&#x27;g&#x27;, 100:&#x27;b&#x27;&#125;</span><br><span class="line">for k, v in residuals.items():</span><br><span class="line">    ax.hist(v,bins=20,label=&#x27;n_estimators: %d&#x27; % k, color=colors[k], alpha=.5);</span><br><span class="line">ax.set_title(&quot;Residuals at Various Numbers of Estimators&quot;)</span><br><span class="line">ax.legend(loc=&#x27;best&#x27;);</span><br></pre></td></tr></table></figure>

<p><img src="http://img.voidcn.com/vcimg/000/005/140/666_75b_e5c.jpg" alt="这里写图片描述"></p>
<p>图像看着有点混乱，但是依然可以看出随着估计器数据的增加，误差在减少。不过，这并不是一成不变的。首先，我们没有交叉检验过，其次，随着估计器数量的增加，训练时间也会变长。现在我们用数据比较小没什么关系，但是如果数据再放大一两倍问题就出来了。</p>
<h2 id="GBR参数设置"><a href="#GBR参数设置" class="headerlink" title="GBR参数设置"></a>GBR参数设置</h2><p>上面例子中GBR的第一个参数是<code>n_estimators</code>，指GBR使用的学习算法的数量。通常，如果你的设备性能更好，可以把<code>n_estimators</code>设置的更大，效果也会更好。还有另外几个参数要说明一下。</p>
<p>你应该在优化其他参数之前先调整<code>max_depth</code>参数。因为每个学习算法都是一颗决策树，<code>max_depth</code>决定了树生成的节点数。选择合适的节点数量可以更好的拟合数据，而更多的节点数可能造成拟合过度。</p>
<p><code>loss</code>参数决定损失函数，也直接影响误差。<code>ls</code>是默认值，表示最小二乘法（least squares）。还有最小绝对值差值，Huber损失和分位数损失（quantiles）等等。</p>

          <br>
<p>dzzxjl</p>
<!--<p><img src='/images/scribble3.png' alt='scribble'></p>-->

        </section>
      </div>
      
      <div class='block'>
  
    <a class='main' href='/'>Home</a>
  
    <a class='main' href='/categories/linux'>Linux</a>
  
    <a class='main' href='/categories/ml'>ML</a>
  
    <a class='main' href='/categories/python'>Python</a>
  
    <a class='main' href='/categories/java'>Java</a>
  
    <a class='main' href='/thoughts'>Thoughts</a>
  
    <a class='main' href='/kmkg'>KmKg</a>
  
    <a class='main' href='/bookcan'>BookCan</a>
  
    <a class='main' href='/links'>Links</a>
  
    <a class='main' href='/about'>About</a>
  
</div>

    </div>
    <footer>
  <span class='muted'>&copy; dzzxjl. 2023. All Rights Reserved.</span><br>
  <a target="_blank" rel="noopener" href='https://github.com/saintwinkle/hexo-theme-scribble' class='muted'>一辈子很短，如白驹过隙，转瞬即逝。而这种心情很长，如高山大川，延绵不绝。</a>
  <br>
  <br>
  <!--<img src='/images/scribble2.png' alt='scribble' />-->
</footer>

  </body>
</html>
