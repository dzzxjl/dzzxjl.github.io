<html>
  <head>
    <title>人工智能发展史 - 十一城</title>
    <link href='/images/fav.png' rel='shortcut icon'>
<link href='/atom.xml' rel='alternate' type='application/rss+xml'>

<link rel="stylesheet" href="/css/style.css">
<link rel="stylesheet" href="/css/highlight.css">
<link rel="stylesheet" href="/css/responsive.css">


<script src="/js/jquery.js"></script>
<script src="/js/basics.js"></script>

<meta content='width=device-width, initial-scale=1.0, user-scalable=no' name='viewport'>
<meta content='text/html; charset=utf-8' http-equiv='content-type'>


  <meta name="generator" content="Hexo 6.3.0"></head>
  <body>
    <header>
  <!--<a id='go-back-home' href='/'><img alt='Home' width='53' height='59'></a>-->
  <br>
  <p>十一城</p>
  <p>跬步千里，小流江海。</p>
</header>

    <div id='container'>
      <div class='block'>
  
    <a class='main' href='/'>Home</a>
  
    <a class='main' href='/categories/linux'>Linux</a>
  
    <a class='main' href='/categories/ml'>ML</a>
  
    <a class='main' href='/categories/python'>Python</a>
  
    <a class='main' href='/categories/java'>Java</a>
  
    <a class='main' href='/thoughts'>Thoughts</a>
  
    <a class='main' href='/kmkg'>KmKg</a>
  
    <a class='main' href='/bookcan'>BookCan</a>
  
    <a class='main' href='/links'>Links</a>
  
    <a class='main' href='/about'>About</a>
  
</div>

      <section class='paging'>
  
    <div class='left'>
      <a href='/2017/11/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/'>
        ‹
      </a>
    </div>
  
  
    <div class='right'>
      <a href='/2017/10/26/%E5%8D%97%E4%BA%AC%E8%BD%AF%E4%BB%B6%E4%B8%8E%E4%BA%92%E8%81%94%E7%BD%91%E8%A1%8C%E4%B8%9A%E5%8F%91%E5%B1%95%E7%A0%94%E7%A9%B6/'>
        ›
      </a>
    </div>
  
</section>

      <div class='content'>
        <section class='post'>
          <h1>
            <div class='date'>2017-11-07</div>
            人工智能发展史
          </h1>
          
          
          
          <hr>


          <h2 id="1955之前"><a href="#1955之前" class="headerlink" title="1955之前"></a>1955之前</h2><ul>
<li>1799 1805 最小二乘法</li>
<li>1936 图灵机</li>
<li>1950 图灵测试</li>
<li>1952 机器学习</li>
</ul>
<h2 id="1955"><a href="#1955" class="headerlink" title="1955"></a>1955</h2><ul>
<li>1950~1960 线性回归</li>
<li>1955 冯诺依曼去世</li>
<li>1956 一场在美国达特茅斯（Dartmouth）大学召开的学术会议</li>
<li>1957 <strong>罗森·布拉特</strong>基于神经感知科学背景提出了第二模型，非常的类似于今天的机器学习模型 perceptron感知机</li>
<li>1960 最小二乘法</li>
</ul>
<h2 id="1965——基于逻辑表示的“符号主义”学习技术"><a href="#1965——基于逻辑表示的“符号主义”学习技术" class="headerlink" title="1965——基于逻辑表示的“符号主义”学习技术"></a>1965——基于逻辑表示的“符号主义”学习技术</h2><ul>
<li>多值逻辑</li>
<li>计算机能证明逻辑</li>
<li>1967 kNN</li>
<li>1969 XOR</li>
</ul>
<h2 id="20世纪60年代中叶到70年代末"><a href="#20世纪60年代中叶到70年代末" class="headerlink" title="20世纪60年代中叶到70年代末"></a>20世纪60年代中叶到70年代末</h2><p>停滞不前的冷静时期</p>
<ul>
<li>1980 在美国的卡内基梅隆大学(CMU)召开了<strong>第一届机器学习国际研讨会</strong></li>
</ul>
<h2 id="二十世纪八十年代-符号主义学习"><a href="#二十世纪八十年代-符号主义学习" class="headerlink" title="二十世纪八十年代-符号主义学习"></a>二十世纪八十年代-符号主义学习</h2><p>理夏德米哈尔斯基</p>
<p>汤姆 米切尔</p>
<p>杰米 卡博内尔</p>
<p>代表包括<strong>决策树</strong>和基于逻辑的学习。但由于学习过程面临的<strong>假设空间太大</strong>、复杂度极高，因此，问题规模稍大就难以有效进行学习，九十年代中期后这方面的研究相对陷入低潮；</p>
<h2 id="1985"><a href="#1985" class="headerlink" title="1985"></a>1985</h2><ul>
<li>模糊逻辑应用于工业、交通</li>
<li>1986 神经网络雄起</li>
<li>遗传算法</li>
</ul>
<h2 id="二十世纪九十年代中期之前-基于神经网络的（联结）连接学习"><a href="#二十世纪九十年代中期之前-基于神经网络的（联结）连接学习" class="headerlink" title="二十世纪九十年代中期之前-基于神经网络的（联结）连接学习"></a>二十世纪九十年代中期之前-基于神经网络的（联结）连接学习</h2><p>神经元模型1943</p>
<p>连接主义在二十世纪五十年代取得了大发展，同时也遇到了很大的障碍，如图灵得主M.Minsky和S.Papert在1969年指出，（当时的）神经网络只能处理<strong>线性分类</strong>，甚至对“异或”这么简单的问题都处理不了。1983年，J.J Hopfield利用神经网络求解“流动推销员问题”这个著名的NP难题取得重大进展，使得连接注意重新受到人们关注。1986年，D.E. Rumelhart等人重新发明了著名的BP（反向传播）算法，产生了深远影响。神经网络学习过程涉及大量参数，而参数的设置缺乏理论指导，主要靠手工“调参”；夸张一点说，参数调节上失之毫厘，学习结果可能谬以千里。</p>
<h2 id="二十世纪九十年代中期-统计机器学习"><a href="#二十世纪九十年代中期-统计机器学习" class="headerlink" title="二十世纪九十年代中期-统计机器学习"></a>二十世纪九十年代中期-统计机器学习</h2><p>代表性技术是<strong>支持向量机</strong>以及更一般的<strong>核方法</strong>。这方面的研究早在二十世纪六七十年代就已开始。统计学习与连接主义学习有密切的联系。支持向量机被普遍接受后，核技巧（kernel rick）被人们用到了机器学习的几乎每个角落，核方法也逐渐成为机器学习的基本内容之一。</p>
<h2 id="二十一世纪初-深度学习"><a href="#二十一世纪初-深度学习" class="headerlink" title="二十一世纪初-深度学习"></a>二十一世纪初-深度学习</h2><p>深度学习，狭义的说就是“很多层”的神经网络。深度学习虽然缺乏严格的理论基础，但它显著的降低了机器学习应用者的门槛，为机器学习技术走向工程实践带来了便利。深度学习火热有两个基本原因：数据大了、计算能力强了。深度学习模型拥有大量数据，若数据样本少，则很容易“过拟合”；如此复杂的模型，如此大的数据样本，若缺乏强力计算设备，根本无法求解。</p>
<hr>
<p>以下内容来自Wikipedia</p>
<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><table>
<thead>
<tr>
<th>Decade</th>
<th>Summary</th>
</tr>
</thead>
<tbody><tr>
<td>&lt;1950s</td>
<td>Statistical methods are discovered and refined.</td>
</tr>
<tr>
<td>1950s</td>
<td>Pioneering <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Machine_learning">machine learning</a> research is conducted using simple algorithms.</td>
</tr>
<tr>
<td>1960s</td>
<td><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Bayesian_method">Bayesian methods</a> are introduced for <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Bayesian_inference">probabilistic inference</a> in machine learning.[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-1">1]</a></td>
</tr>
<tr>
<td>1970s</td>
<td>‘<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/AI_Winter">AI Winter</a>‘ caused by pessimism about machine learning effectiveness.</td>
</tr>
<tr>
<td>1980s</td>
<td>Rediscovery of <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Backpropagation">backpropagation</a> causes a resurgence in machine learning research.</td>
</tr>
<tr>
<td>1990s</td>
<td>Work on machine learning shifts from a knowledge-driven approach to a data-driven approach. Scientists begin creating programs for computers to analyze large amounts of data and draw conclusions – or “learn” – from the results.[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-Marr-2">2]</a> <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Support_vector_machines">Support vector machines</a> (SVMs) and [<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-3">3]</a><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Recurrent_neural_networks">recurrent neural networks</a> (RNNs) become popular. The fields of [<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-4">4]</a>computational complexity via neural networks and super-Turing computation started.</td>
</tr>
<tr>
<td>2000s</td>
<td>Support Vector Clustering [<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-5">5]</a> and other <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Kernel_methods">Kernel methods</a> [<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-6">6]</a> and unsupervised machine learning methods become widespread.[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-7">7]</a></td>
</tr>
<tr>
<td>2010s</td>
<td><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Deep_learning">Deep learning</a> becomes feasible, which leads to machine learning becoming integral to many widely used software services and applications.</td>
</tr>
</tbody></table>
<h2 id="Timeline"><a href="#Timeline" class="headerlink" title="Timeline"></a>Timeline</h2><table>
<thead>
<tr>
<th>Year</th>
<th>Event type</th>
<th>Caption</th>
<th>Event</th>
</tr>
</thead>
<tbody><tr>
<td>1763</td>
<td>Discovery</td>
<td>The Underpinnings of <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Bayes%27_theorem">Bayes’ Theorem</a></td>
<td><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Thomas_Bayes">Thomas Bayes</a>‘s work <em>An Essay towards solving a Problem in the Doctrine of Chances</em> is published two years after his death, having been amended and edited by a friend of Bayes, <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Richard_Price">Richard Price</a>.[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-8">8]</a> The essay presents work which underpins <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Bayes_theorem">Bayes theorem</a>.</td>
</tr>
<tr>
<td>1805</td>
<td>Discovery</td>
<td>Least Squares</td>
<td><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Adrien-Marie_Legendre">Adrien-Marie Legendre</a> describes the “méthode des moindres carrés”, known in English as the <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Least_squares">least squares</a> method.[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-9">9]</a> The least squares method is used widely in <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Data_fitting">data fitting</a>.</td>
</tr>
<tr>
<td>1812</td>
<td></td>
<td><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Bayes%27_theorem">Bayes’ Theorem</a></td>
<td><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Pierre-Simon_Laplace">Pierre-Simon Laplace</a> publishes <em>Théorie Analytique des Probabilités</em>, in which he expands upon the work of Bayes and defines what is now known as <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Bayes%27_Theorem">Bayes’ Theorem</a>.[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-10">10]</a></td>
</tr>
<tr>
<td>1913</td>
<td>Discovery</td>
<td>Markov Chains</td>
<td><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Andrey_Markov">Andrey Markov</a> first describes techniques he used to analyse a poem. The techniques later become known as <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Markov_chains">Markov chains</a>.[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-11">11]</a></td>
</tr>
<tr>
<td>1950</td>
<td></td>
<td>Turing’s Learning Machine</td>
<td><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Alan_Turing">Alan Turing</a> proposes a ‘learning machine’ that could learn and become artificially intelligent. Turing’s specific proposal foreshadows <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Genetic_algorithms">genetic algorithms</a>.[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-12">12]</a></td>
</tr>
<tr>
<td>1951</td>
<td></td>
<td>First Neural Network Machine</td>
<td><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Marvin_Minsky">Marvin Minsky</a> and Dean Edmonds build the first neural network machine, able to learn, the <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Stochastic_neural_analog_reinforcement_calculator">SNARC</a>.[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-13">13]</a></td>
</tr>
<tr>
<td>1952</td>
<td></td>
<td>Machines Playing Checkers</td>
<td><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Arthur_Samuel">Arthur Samuel</a> joins IBM’s Poughkeepsie Laboratory and begins working on some of the very first machine learning programs, first creating programs that play <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Checkers">checkers</a>.[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-aaai-14">14]</a></td>
</tr>
<tr>
<td>1957</td>
<td>Discovery</td>
<td>Perceptron</td>
<td><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Frank_Rosenblatt">Frank Rosenblatt</a> invents the <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Perceptron">perceptron</a> while working at the <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Cornell_Aeronautical_Laboratory">Cornell Aeronautical Laboratory</a>.[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-15">15]</a> The invention of the perceptron generated a great deal of excitement and was widely covered in the media.[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-16">16]</a></td>
</tr>
<tr>
<td>1963</td>
<td>Achievement</td>
<td>Machines Playing Tic-Tac-Toe</td>
<td><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Donald_Michie">Donald Michie</a> creates a ‘machine’ consisting of 304 match boxes and beads, which uses <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning</a> to play <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Tic-tac-toe">Tic-tac-toe</a> (also known as noughts and crosses).[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-17">17]</a></td>
</tr>
<tr>
<td>1967</td>
<td></td>
<td>Nearest Neighbor</td>
<td>The nearest neighbor algorithm was created, which is the start of basic pattern recognition. The algorithm was used to map routes.[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-Marr-2">2]</a></td>
</tr>
<tr>
<td>1969</td>
<td></td>
<td>Limitations of Neural Networks</td>
<td><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Marvin_Minsky">Marvin Minsky</a> and <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Seymour_Papert">Seymour Papert</a> publish their book <em>Perceptrons</em>, describing some of the limitations of perceptrons and neural networks. The interpretation that the book shows that neural networks are fundamentally limited is seen as a hindrance for research into neural networks.[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-18">18]</a>[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-19">19]</a></td>
</tr>
<tr>
<td>1970</td>
<td></td>
<td>Automatic Differentation (Backpropagation)</td>
<td><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Seppo_Linnainmaa">Seppo Linnainmaa</a> publishes the general method for automatic differentiation (AD) of discrete connected networks of nested differentiable functions.[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-lin1970-20">20]</a>[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-lin1976-21">21]</a> This corresponds to the modern version of backpropagation, but is not yet named as such.[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-grie2012-22">22]</a>[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-grie2008-23">23]</a>[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-schmidhuber2015-24">24]</a>[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-scholarpedia2015-25">25]</a></td>
</tr>
<tr>
<td>1972</td>
<td>Discovery</td>
<td>Term frequency–inverse document frequency (TF-IDF)</td>
<td><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Karen_Sp%C3%A4rck_Jones">Karen Spärck Jones</a> publishes the concept of <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf">TF-IDF</a>, a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-26">26]</a> 83% of text-based recommender systems in the domain of digital libraries use tf-idf.[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-27">27]</a></td>
</tr>
<tr>
<td>1979</td>
<td></td>
<td>Stanford Cart</td>
<td>Students at Stanford University develop a cart that can navigate and avoid obstacles in a room.[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-Marr-2">2]</a></td>
</tr>
<tr>
<td>1980</td>
<td>Discovery</td>
<td>Neocognitron</td>
<td><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Kunihiko_Fukushima">Kunihiko Fukushima</a> first publishes his work on the <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Neocognitron">neocognitron</a>, a type of <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Artificial_neural_network">artificial neural network</a> (ANN).[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-28">28]</a> <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Neocognitron">Neocognition</a> later inspires <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Convolutional_neural_network">convolutional neural networks</a> (CNNs).[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-29">29]</a></td>
</tr>
<tr>
<td>1981</td>
<td></td>
<td>Explanation Based Learning</td>
<td>Gerald Dejong introduces Explanation Based Learning, where a computer algorithm analyses data and creates a general rule it can follow and discard unimportant data.[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-Marr-2">2]</a></td>
</tr>
<tr>
<td>1982</td>
<td>Discovery</td>
<td>Recurrent Neural Network</td>
<td><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/John_Hopfield">John Hopfield</a> popularizes <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Hopfield_networks">Hopfield networks</a>, a type of <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Recurrent_neural_network">recurrent neural network</a> that can serve as <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Content-addressable_memory">content-addressable memory</a> systems.[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-30">30]</a></td>
</tr>
<tr>
<td>1985</td>
<td></td>
<td>NetTalk</td>
<td>A program that learns to pronounce words the same way a baby does, is developed by Terry Sejnowski.[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-Marr-2">2]</a></td>
</tr>
<tr>
<td>1986</td>
<td>Discovery</td>
<td>Backpropagation</td>
<td>The process of <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Backpropagation">backpropagation</a> is described by <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/David_Rumelhart">David Rumelhart</a>, <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Geoff_Hinton">Geoff Hinton</a> and <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Ronald_J._Williams">Ronald J. Williams</a>.[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-31">31]</a></td>
</tr>
<tr>
<td>1989</td>
<td>Discovery</td>
<td>Reinforcement Learning</td>
<td>Christopher Watkins develops <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Q-learning">Q-learning</a>, which greatly improves the practicality and feasibility of <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning</a>.[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-32">32]</a></td>
</tr>
<tr>
<td>1989</td>
<td>Commercialization</td>
<td>Commercialization of Machine Learning on Personal Computers</td>
<td>Axcelis, Inc. releases <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Evolver_(software)">Evolver</a>, the first software package to commercialize the use of genetic algorithms on personal computers.[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-33">33]</a></td>
</tr>
<tr>
<td>1992</td>
<td>Achievement</td>
<td>Machines Playing Backgammon</td>
<td>Gerald Tesauro develops <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/TD-Gammon">TD-Gammon</a>, a computer <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Backgammon">backgammon</a> program that uses an <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Artificial_neural_network">artificial neural network</a> trained using <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Temporal-difference_learning">temporal-difference learning</a> (hence the ‘TD’ in the name). TD-Gammon is able to rival, but not consistently surpass, the abilities of top human backgammon players.[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-34">34]</a></td>
</tr>
<tr>
<td>1995</td>
<td>Discovery</td>
<td>Random Forest Algorithm</td>
<td>Tin Kam Ho publishes a paper describing <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Random_forest">random decision forests</a>.[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-35">35]</a></td>
</tr>
<tr>
<td>1995</td>
<td>Discovery</td>
<td>Support Vector Machines</td>
<td><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Corinna_Cortes">Corinna Cortes</a> and <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Vladimir_Vapnik">Vladimir Vapnik</a> publish their work on <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Support_vector_machines">support vector machines</a>.[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-bhml-36">36]</a>[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-37">37]</a></td>
</tr>
<tr>
<td>1997</td>
<td>Achievement</td>
<td>IBM Deep Blue Beats Kasparov</td>
<td>IBM’s <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)">Deep Blue</a> beats the world champion at chess.[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-Marr-2">2]</a></td>
</tr>
<tr>
<td>1997</td>
<td>Discovery</td>
<td>LSTM</td>
<td><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Sepp_Hochreiter">Sepp Hochreiter</a> and <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/J%C3%BCrgen_Schmidhuber">Jürgen Schmidhuber</a> invent <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Long_short-term_memory">long short-term memory</a> (LSTM) recurrent neural networks,[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-38">38]</a> greatly improving the efficiency and practicality of recurrent neural networks.</td>
</tr>
<tr>
<td>1998</td>
<td></td>
<td>MNIST database</td>
<td>A team led by <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Yann_LeCun">Yann LeCun</a> releases the <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/MNIST_database">MNIST database</a>, a dataset comprising a mix of handwritten digits from <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/American_Census_Bureau">American Census Bureau</a> employees and American high school students.[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-39">39]</a> The MNIST database has since become a benchmark for evaluating handwriting recognition.</td>
</tr>
<tr>
<td>2002</td>
<td></td>
<td>Torch Machine Learning Library</td>
<td><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Torch_(machine_learning)">Torch</a>, a software library for machine learning, is first released.[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-40">40]</a></td>
</tr>
<tr>
<td>2006</td>
<td></td>
<td>The Netflix Prize</td>
<td>The <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Netflix_Prize">Netflix Prize</a> competition is launched by <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Netflix">Netflix</a>. The aim of the competition was to use machine learning to beat Netflix’s own recommendation software’s accuracy in predicting a user’s rating for a film given their ratings for previous films by at least 10%.[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-41">41]</a> The prize was won in 2009.</td>
</tr>
<tr>
<td>2009</td>
<td>Achievement</td>
<td>ImageNet</td>
<td><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/ImageNet">ImageNet</a> is created. ImageNet is a large visual database envisioned by <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Fei-Fei_Li">Fei-Fei Li</a> from Stanford University, who realized that the best machine learning algorithms wouldn’t work well if the data didn’t reflect the real world.[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-42">42]</a> For many, ImageNet was the catalyst for the AI boom[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-43">43]</a>of the 21st century.</td>
</tr>
<tr>
<td>2010</td>
<td></td>
<td>Kaggle Competition</td>
<td><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Kaggle">Kaggle</a>, a website that serves as a platform for machine learning competitions, is launched.[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-44">44]</a></td>
</tr>
<tr>
<td>2011</td>
<td>Achievement</td>
<td>Beating Humans in Jeopardy</td>
<td>Using a combination of machine learning, <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a> and information retrieval techniques, <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/IBM">IBM</a>‘s <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Watson_(computer)">Watson</a> beats two human champions in a <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Jeopardy!">Jeopardy!</a> competition.[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-45">45]</a></td>
</tr>
<tr>
<td>2012</td>
<td>Achievement</td>
<td>Recognizing Cats on YouTube</td>
<td>The <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Google_Brain">Google Brain</a> team, led by <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Andrew_Ng">Andrew Ng</a> and <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Jeff_Dean_(computer_scientist)">Jeff Dean</a>, create a neural network that learns to recognize cats by watching unlabeled images taken from frames of <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/YouTube">YouTube</a> videos.[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-46">46]</a>[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-47">47]</a></td>
</tr>
<tr>
<td>2014</td>
<td></td>
<td>Leap in Face Recognition</td>
<td><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Facebook">Facebook</a> researchers publish their work on <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/DeepFace">DeepFace</a>, a system that uses neural networks that identifies faces with 97.35% accuracy. The results are an improvement of more than 27% over previous systems and rivals human performance.[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-48">48]</a></td>
</tr>
<tr>
<td>2014</td>
<td></td>
<td>Sibyl</td>
<td>Researchers from <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Google">Google</a> detail their work on Sibyl,[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-49">49]</a> a proprietary platform for massively parallel machine learning used internally by Google to make predictions about user behavior and provide recommendations.[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-50">50]</a></td>
</tr>
<tr>
<td>2016</td>
<td>Achievement</td>
<td>Beating Humans in Go</td>
<td>Google’s <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/AlphaGo">AlphaGo</a> program becomes the first <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Computer_Go">Computer Go</a> program to beat an unhandicapped professional human player[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-51">51]</a> using a combination of machine learning and tree search techniques.[<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning#cite_note-52">52]</a> Later improved as <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/AlphaGo_Zero">AlphaGo Zero</a> and then in 2017 generalized to Chess and more two-player games with <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/AlphaZero">AlphaZero</a>.</td>
</tr>
</tbody></table>
<h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><ul>
<li><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/History_of_artificial_intelligence">https://en.wikipedia.org/wiki/History_of_artificial_intelligence</a></li>
<li><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Timeline_of_machine_learning">https://en.wikipedia.org/wiki/Timeline_of_machine_learning</a></li>
<li>6段Python代码刻画深度学习历史：从最小二乘法到深度神经网络</li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/33089702">https://zhuanlan.zhihu.com/p/33089702</a></li>
<li><a target="_blank" rel="noopener" href="https://www.leiphone.com/news/201609/jzfVzJ49LIWJegku.html">https://www.leiphone.com/news/201609/jzfVzJ49LIWJegku.html</a></li>
</ul>

          <br>
<p>dzzxjl</p>
<!--<p><img src='/images/scribble3.png' alt='scribble'></p>-->

        </section>
      </div>
      
      <div class='block'>
  
    <a class='main' href='/'>Home</a>
  
    <a class='main' href='/categories/linux'>Linux</a>
  
    <a class='main' href='/categories/ml'>ML</a>
  
    <a class='main' href='/categories/python'>Python</a>
  
    <a class='main' href='/categories/java'>Java</a>
  
    <a class='main' href='/thoughts'>Thoughts</a>
  
    <a class='main' href='/kmkg'>KmKg</a>
  
    <a class='main' href='/bookcan'>BookCan</a>
  
    <a class='main' href='/links'>Links</a>
  
    <a class='main' href='/about'>About</a>
  
</div>

    </div>
    <footer>
  <span class='muted'>&copy; dzzxjl. 2023. All Rights Reserved.</span><br>
  <a target="_blank" rel="noopener" href='https://github.com/saintwinkle/hexo-theme-scribble' class='muted'>一辈子很短，如白驹过隙，转瞬即逝。而这种心情很长，如高山大川，延绵不绝。</a>
  <br>
  <br>
  <!--<img src='/images/scribble2.png' alt='scribble' />-->
</footer>

  </body>
</html>
